import os
import shutil
import os.path as path
import csv
import pandas as pd

"""
This module is to pre-process tables generated by LLMs(ChatGPT/Gemini) to 
normalize them to facilitate benchmark.
"""

PK_COLUMNS = [
    "Drug name",
    "Analyte",
    "Specimen",
    "Population",
    "Pregnancy stage",
    "Summary statistics",
    "Parameter type",
    "Value",
    "Unit",
    "Subject N",
    "Variation value",
    "Variation type",
    "P value",
    "Interval type",
    "Lower limit",
    "High limit",
]
LOWER_PK_COLUMNS = [
    "drug name",
    "analyte",
    "specimen",
    "population",
    "pregnancy stage",
    "summary statistics",
    "parameter type",
    "value",
    "unit",
    "subject n",
    "variation value",
    "variation type",
    "p value",
    "interval type",
    "lower limit",
    "high limit",
]
PK_COLUMNS_MAP = [
    ("P-value", "P value"),
    ("Subjectsn", "Subject N"),
    ("Subjects n", "Subject N"),
    ("Interval low", "Lower limit"),
    ("interval high", "High limit"),
    ("Variability statistic", "Variation type"),
    ("Summary Statistic", "Summary statistics"),
    ("Parameter unit", "Unit"),
    ("Parameter statistic", "Summary statistics"),
    ("Parameter value", "Value"),
    ("Lower bound", "Lower limit"),
    ("Upper bound", "High limit"),
]
    
def ensure_columns(df_table: pd.DataFrame) -> pd.DataFrame:
    """ Normalize pk summary columns """
    col_map = {}
    for item in PK_COLUMNS_MAP:
        col_map[item[0]] = item[1]

    df_table = df_table.rename(columns=col_map)
    df_table = df_table[df_table.columns.intersection(PK_COLUMNS)]

    return df_table

def ensure_NO_column(df_table: pd.DataFrame) -> pd.DataFrame:
    """ Ensure the first column is "NO." column """
    if df_table.columns[0].lower() == LOWER_PK_COLUMNS[0]:
        # No "NO." column
        df_table.insert(0, "NO.", range(len(df_table)))
    
    return df_table
    
def preprocess_pk_summary_table(csv_file: str) -> pd.DataFrame:
    """ preprocess pk summary table """
    df_table = pd.read_csv(csv_file)
    df_table = ensure_columns(df_table)
    df_table = ensure_NO_column(df_table)    

    return df_table

def preprocess_PK_csv_file(pk_csv_file: str):
    bn, extname = path.splitext(pk_csv_file)
    orig_file = f"{bn}-original{extname}"
    try:
        if not os.path.exists(orig_file):
            shutil.copyfile(pk_csv_file, orig_file)
    except Exception as e:
        print(str(e))
        return False

    dst_file = pk_csv_file
    output_df = preprocess_pk_summary_table(orig_file)

    # before write to csv file, remove the first column,
    output_df = output_df.iloc[:, 1:]
    output_df.to_csv(dst_file, sep=",")
    return True


def preprocess_PK_csv_files(pk_csv_files: list[str]):
    for f in pk_csv_files:
        res = preprocess_PK_csv_file(f)
        if not res:
            print(f"Failed to pre-process file: {f}")


def process_single_file():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("csv_file", help="csv file path to be pre-processed")
    args = vars(parser.parse_args())
    print(args)
    preprocess_PK_csv_file(args["csv_file"])
    return


def process_multiple_files():
    pk_files = [
        "./benchmark/data/pk-summary/baseline/A/18394772_baseline.csv",
    ]
    preprocess_PK_csv_files(pk_files)


if __name__ == "__main__":
    process_single_file()
