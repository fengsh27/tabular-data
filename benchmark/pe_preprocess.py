import os
from typing import List, Optional
import os.path as path
import csv
import shutil

"""
This module is to pre-process tables generated by LLMs(ChatGPT/Gemini) to 
normalize them to facilitate benchmark.
"""

PE_COLUMNS = [
    "Characteristic/risk factor",
    "Exposure",
    "Outcomes",
    "Statistic",
    "Value",
    "Unit",
    "Variability statistic",
    "Variability value",
    "Interval type",
    "Interval low",
    "Interval high",
    "P value",
]
PK_COLUMNS_MAP = [
    ("P-value", "P value"),
    ("Interval Low Bound", "Lower limit"),
    ("Interval High Bound", "High limit"),
    ("Intervale High Bound", "High limit"),
]

def get_lower_column_name():
    cols = []
    for v in PE_COLUMNS:
        cols.append(v.lower())
    return cols

def process_1st_column(rows: List[List[str]]):
    lower_cols = get_lower_column_name()
    headers = rows[0]
    if headers[0].lower() == lower_cols[0]:
        # no NO. column in table
        prc_rows = []
        for ix, row in enumerate(rows):
            prc_row = []
            if ix == 0:
                prc_row.append('')
            else:
                prc_row.append(ix-1)
            prc_row = prc_row + row
            prc_rows.append(prc_row)
        return prc_rows

    for ix, row in enumerate(rows):
        if ix == 0:
            row[0] = ""
        else:
            row[0] = ix - 1
    
    return rows

def process_column_names(rows: List[List[str]]):
    lower_cols = get_lower_column_name()
    unknow_col_index = []

    for ix in range(len(rows[0])):
        column = rows[0][ix]
        if ix == 0:
            continue
        found = False
        if len(column.strip()) == 0:
            print(f"Error: the {ix}th column is empty")
            unknow_col_index.append(ix)
            continue
        # Normalize column names
        try:
            col_ix = lower_cols.index(column.strip().lower())
            rows[0][ix] = PE_COLUMNS[col_ix]
            found = True
        except ValueError:
            found = False

        # Map column name
        if not found:
            pair = next(( x for x in PK_COLUMNS_MAP if x[0].lower() == column.strip().lower() ), None)
            if pair is not None:
                rows[0][ix] = pair[1]
                found = True
            # for pk_col_map in PK_COLUMNS_MAP:
            #     if column.strip().lower() == pk_col_map[0].lower():
            #         rows[0][ix] = pk_col_map[1]
            #         found = True
            #         break
        if not found:
            print(f"Error: can't find column {column}")
            unknow_col_index.append(ix)
    
    if len(unknow_col_index) == 0:
        return rows
    
    processed_rows = []
    for row in rows:
        prcssed_row = []
        for ix in range(len(row)):
            if ix not in unknow_col_index:
                prcssed_row.append(row[ix])
        processed_rows.append(prcssed_row)

    return processed_rows


def preprocess_table(csv_file):
    with open(csv_file, "r") as fobj:
        reader = csv.reader(fobj)
        csv_rows = list(reader)
        stripped_rows = [[val.replace('\ufeff', '') for val in row] for row in csv_rows]
        
        rows = process_1st_column(stripped_rows)
        rows = process_column_names(rows)

        return rows
    

def preprocess_PK_csv_file(pk_csv_file: str):
    bn, extname = path.splitext(pk_csv_file)
    orig_file = f"{bn}-original{extname}"
    try:
        shutil.copyfile(pk_csv_file, orig_file)
    except Exception as e:
        print(str(e))
        return False

    dst_file = pk_csv_file
    rows = preprocess_table(orig_file)
    with open(dst_file, "w") as fobj:
        writer = csv.writer(fobj)
        writer.writerows(rows)
        return True

def preprocess_PK_csv_files(pk_csv_files: List[str]):
    for f in pk_csv_files:
        res = preprocess_PK_csv_file(f)
        if not res:
            print(f"Failed to pre-process file: {f}")

def process_triple_files():
    PK_PMIDs = [
        "15930210",
        "18782787",
        "30308427",
        "33864754",
        "34024233",
        "34083820",
        "34741059",
        "35296792",
        "35997979",
        "36396314",
    ]
    pe_files = []
    for pe_id in PK_PMIDs:
        baseline = f"./benchmark/pe/{pe_id}_baseline.csv"
        if path.exists(baseline):
            pe_files.append(baseline)
        gpt4o = f"./benchmark/pe/{pe_id}_gpt4o.csv"
        if path.exists(gpt4o):
            pe_files.append(gpt4o)
        gemini = f"./benchmark/pe/{pe_id}_gemini15.csv"
        if path.exists(gemini):
            pe_files.append(gemini)
    preprocess_PK_csv_files(pe_files)

def process_single_files():
    PE_FILES = [
        "30308427_gemini15.csv",
        "34741059_gemini15.csv",
        "35296792_gemini15.csv"
    ]
    pe_files = []
    for f in PE_FILES:
        fn = f"./benchmark/pe/{f}"
        pe_files.append(fn)
    preprocess_PK_csv_files(pe_files)
        

if __name__ == "__main__":
    process_single_files()
    

